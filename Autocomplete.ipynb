{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autocomplete.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JPjAIaQ7RLA"
      },
      "source": [
        "# Auto Complete\r\n",
        "\r\n",
        "For this project, we are making a auto completer for implementation of Information Retrieval. We make this auto complete because we think this is a simple yet very useful tools for publics.\r\n",
        "\r\n",
        "For the implementation of the Information Retrieval, here we use TF-IDF and cosine similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOnr42CP8l4u"
      },
      "source": [
        "in this part, we are importing all of the packages and libraries that will be used in the future. Which here, we use json as our dataset for training, the json it self contains features like message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k048kkSE8idH"
      },
      "source": [
        "import json\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from pandas.io.json import json_normalize\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm3DiUB2JB_V"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdYcGMGA9Gsk"
      },
      "source": [
        "Here, we read \"sample_conversation.json\" for our training dataset, after the data is loaded, we are doing preprocessing. The preprocessing is to get the specified messages from the json. And to do that, we need to form a dataframe, and also renaming the column name for better use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmB4vdY6J5L1",
        "outputId": "6666eb00-0c33-498a-c108-4fc32ceb6e9f"
      },
      "source": [
        "df = pd.read_json('/content/sample_conversations.json')\r\n",
        "\r\n",
        "for column in ['Issues']:\r\n",
        "  column_as_df = json_normalize(df[column])\r\n",
        "  column_as_df.columns = [str(column+\"_\"+subcolumn) for subcolumn in column_as_df.columns]\r\n",
        "  df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\r\n",
        "\r\n",
        "df = pd.DataFrame([dict(y, index=i) for i, x in enumerate(df['Issues_Messages'].values.tolist()) for y in x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tLrB8jf9j5-"
      },
      "source": [
        "splitDataFrameList is a function to split the message data. the given data will be splited by a certain seperator given to the function. the process include splitting the messages into seperated rows of data. which in the future, the messages will be splitted by punctuation (. , ? ! ;). this process is needed because we need to know what the meaning of the messages word by word, so the program can return the result that make sense corresponding to our needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6kYdbtBQATA"
      },
      "source": [
        "def splitDataFrameList(df,target_column,separator):\r\n",
        "  def split_text(line, separator):\r\n",
        "      splited_line =  [e+d for e in line.split(separator) if e]\r\n",
        "      return splited_line\r\n",
        "\r\n",
        "  def splitListToRows(row,row_accumulator,target_column,separator):\r\n",
        "      split_row = row[target_column].split(separator)\r\n",
        "      for s in split_row:\r\n",
        "          new_row = row.to_dict()\r\n",
        "          new_row[target_column] = s\r\n",
        "          row_accumulator.append(new_row)\r\n",
        "  new_rows = []\r\n",
        "  df.apply(splitListToRows,axis=1,args = (new_rows,target_column,separator))\r\n",
        "  new_df = pd.DataFrame(new_rows)\r\n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNIOkTav_HvO"
      },
      "source": [
        "process_data is a function for processing the data. First step we do is to seperate the messages using previously created function splitDataFrameList. after that, in here we are using regex to justify the sentence structure. some justification we did are :\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "1.   Adding space into each word\r\n",
        "2.   Removing \".\" from all messages\r\n",
        "3.   The same with the first one\r\n",
        "4.   Replacing \"i\" with \"I\" for every \"i\" that represents a subject\r\n",
        "5.   Replacing \" ?\" with \"?\" only\r\n",
        "6.   Replacing \" !\" with \"!\" only\r\n",
        "7.   Replacing \" .\" with \".\" only\r\n",
        "8.   Replacing \"OK\" with \"Ok\"\r\n",
        "9.   Transform first character of a sentence into uppercase\r\n",
        "10.  Appending \"?\" for every question sentence which doesn't have \"?\" from the begining\r\n",
        "\r\n",
        "After that, we are removing all words that have length lesser than or equals 2, because most of the words which have length lesser than or equals 2 doesn't play a important role to determine the meaning of the sentence.\r\n",
        "\r\n",
        "After that, we are dropping duplicates words.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs2o_guAO6Xl"
      },
      "source": [
        "def process_data(new_df):\r\n",
        "  new_df = new_df[new_df.IsFromCustomer==False]\r\n",
        "  \r\n",
        "  for sep in ['. ',', ','? ', '! ', '; ']:\r\n",
        "      new_df = splitDataFrameList(new_df, 'Text', sep)\r\n",
        "      \r\n",
        "  new_df['Text']=new_df['Text'].apply(lambda x: \" \".join(x.split()))\r\n",
        "  new_df['Text']=new_df['Text'].apply(lambda x: x.strip(\".\"))\r\n",
        "  new_df['Text']=new_df['Text'].apply(lambda x: \" \".join(x.split()))\r\n",
        "  new_df['Text']=new_df['Text'].apply(lambda x: x.replace(' i ',' I '))\r\n",
        "  new_df['Text']=new_df['Text'].apply(lambda x: x.replace(' ?','?'))\r\n",
        "  new_df['Text']=new_df['Text'].apply(lambda x: x.replace(' !','!'))\r\n",
        "  new_df['Text']=new_df['Text'].apply(lambda x: x.replace(' .','.'))\r\n",
        "  new_df['Text']=new_df['Text'].apply(lambda x: x.replace('OK','Ok'))\r\n",
        "  new_df['Text']=new_df['Text'].apply(lambda x: x[0].upper()+x[1:])\r\n",
        "  new_df['Text']=new_df['Text'].apply(lambda x: x+\"?\" if re.search(r'^(Wh|How).+([^?])$',x) else x)\r\n",
        "  \r\n",
        "  new_df['nb_words'] = new_df['Text'].apply(lambda x: len(str(x).split(' ')))\r\n",
        "  new_df = new_df[new_df['nb_words']>2]\r\n",
        "  \r\n",
        "  new_df['Counts'] = new_df.groupby(['Text'])['Text'].transform('count')\r\n",
        "  \r\n",
        "  new_df = new_df.drop_duplicates(subset=['Text'], keep='last')\r\n",
        "  \r\n",
        "  new_df = new_df.reset_index(drop=True)\r\n",
        "  print(new_df.shape)  \r\n",
        "  \r\n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnz_fAWICUtS"
      },
      "source": [
        "calc_matrice is a function to create a model of Tf-IDF and also a matrice of TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2-pP-hcQTO6"
      },
      "source": [
        "def calc_matrice(df):\r\n",
        "  model_tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 5), min_df=0)\r\n",
        "  tfidf_matrice = model_tf.fit_transform(df['Text'])\r\n",
        "  print(\"tfidf_matrice \", tfidf_matrice.shape)\r\n",
        "  return model_tf, tfidf_matrice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GauHCGIeDC1L"
      },
      "source": [
        "generate_completions is a function to create a suggestion. process included are first, we defining the weights for each data in our dataframe by applying 1 + log10 of count words. after that, counting the cosine similarity scores. After we got the scores of cosine similarity, we sort all of the possibles words and get top 3 words from our dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD-KapPvQWkw"
      },
      "source": [
        "def generate_completions(prefix_string, data, model_tf, tfidf_matrice):\r\n",
        "        \r\n",
        "  prefix_string = str(prefix_string)\r\n",
        "  new_df = data.reset_index(drop=True)\r\n",
        "  weights = new_df['Counts'].apply(lambda x: 1+ np.log1p(x)).values\r\n",
        "\r\n",
        "  tfidf_matrice_spelling = model_tf.transform([prefix_string])\r\n",
        "\r\n",
        "  cosine_similarite = linear_kernel(tfidf_matrice, tfidf_matrice_spelling)\r\n",
        "  \r\n",
        "  similarity_scores = list(enumerate(cosine_similarite))\r\n",
        "  similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\r\n",
        "  similarity_scores = similarity_scores[0:10]\r\n",
        "\r\n",
        "  similarity_scores = [i for i in similarity_scores]\r\n",
        "  similarity_indices = [i[0] for i in similarity_scores]\r\n",
        "\r\n",
        "  for i in range(len(similarity_scores)):\r\n",
        "      similarity_scores[i][1][0]=similarity_scores[i][1][0]*weights[similarity_indices][i]\r\n",
        "\r\n",
        "  similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\r\n",
        "  similarity_scores = similarity_scores[0:3]\r\n",
        "  similarity_indices_w = [i[0] for i in similarity_scores]\r\n",
        "  \r\n",
        "  return new_df.loc[similarity_indices_w]['Text'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fps4-1LbQpti"
      },
      "source": [
        "# Main Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2VW7QkkQ1a4",
        "outputId": "52262d76-71f7-496a-c6de-3fa37968bf8b"
      },
      "source": [
        "new_df = process_data(df)\r\n",
        "new_df.shape, new_df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8560, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8560, 5),\n",
              " Index(['IsFromCustomer', 'Text', 'index', 'nb_words', 'Counts'], dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY1c4cp7RO_K",
        "outputId": "86745595-1ba1-46de-80d9-1d1fbb797ef2"
      },
      "source": [
        "model_tf, tfidf_matrice = calc_matrice(new_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfidf_matrice  (8560, 99397)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERNxHGpXRSRI",
        "outputId": "115bafe7-bf92-4cf0-a5dd-40c2b0a107ba"
      },
      "source": [
        "prefix = 'Service'\r\n",
        "\r\n",
        "print(prefix,\"    \\n \")\r\n",
        "\r\n",
        "generate_completions(prefix, new_df, model_tf,tfidf_matrice)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Service     \n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Your service is great', 'Enjoy your new service!', 'Has service restored?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}